# ======================= ui.py (local model + YT/Reddit + 4 ch·∫ø ƒë·ªô) =======================
import streamlit as st
import time
import matplotlib.pyplot as plt

# begin: imports ph·ª• tr·ª£ suy lu·∫≠n & crawl 
import os, sys
import numpy as np
from collections import Counter
from pathlib import Path
from wordcloud import WordCloud
from dotenv import load_dotenv; load_dotenv()

# Paths & imports cho c·∫•u tr√∫c ph·∫≥ng (kh√¥ng c√≤n frontend/backend) ====
ROOT = Path(__file__).resolve().parent       # .../sentiment-project
sys.path.append(str(ROOT))                   # ƒë·ªÉ import ƒë∆∞·ª£c crawl_data/*

# Crawler modules n·∫±m ngay trong folder crawl_data/
try:
    from crawl_data.crawl_cmt_from_ytb import Crawler
    from crawl_data.crawl_reddit import CrawlReddit
    _CRAWLER_OK = True
except Exception:
    _CRAWLER_OK = False

# begin: load tokenizer + model .h5 (cache 1 l·∫ßn)
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.text import tokenizer_from_json
from tensorflow.keras.preprocessing.sequence import pad_sequences

# ƒê∆∞·ªùng d·∫´n model/tokenizer trong notebook/models/
MODEL_PATH     = ROOT / "notebook" / "models" / "btlpython3.h5"
TOKENIZER_PATH = ROOT / "notebook" / "models" / "tokenizer.json"
MAX_LEN = 30  # kh·ªõp l√∫c train

@st.cache_resource(show_spinner=False)
def load_artifacts():
    if not TOKENIZER_PATH.exists() or not MODEL_PATH.exists():
        raise FileNotFoundError(
            f"Kh√¥ng t√¨m th·∫•y model/tokenizer:\n- {MODEL_PATH}\n- {TOKENIZER_PATH}\n"
            "H√£y ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n!"
        )
    with open(TOKENIZER_PATH, "r", encoding="utf-8") as f:
        tok = tokenizer_from_json(f.read())
    mdl = load_model(MODEL_PATH)
    return tok, mdl

def predict_local(comments):
    tok, mdl = load_artifacts()
    seq = tok.texts_to_sequences(comments)
    pad = pad_sequences(seq, padding="post", truncating="post", maxlen=MAX_LEN)
    proba = mdl.predict(pad, verbose=0)
    idx = np.argmax(proba, axis=1)
    idx2vi = {0: "Ti√™u c·ª±c", 1: "Trung l·∫≠p", 2: "T√≠ch c·ª±c"}  # theo th·ª© t·ª± training c·ªßa b·∫°n
    return [idx2vi[int(i)] for i in idx]
# --- end ---

# --- C·∫•u h√¨nh trang ---
st.set_page_config(page_title="Sentiment Analysis App", page_icon="üòä", layout="centered")

# --- CSS tu·ª≥ ch·ªânh ---
st.markdown("""
<style>
body {background: linear-gradient(135deg, #f8f9fa, #e3f2fd);}
.stTextArea textarea, .stTextInput input {
    border: 2px solid #4e9af1 !important; border-radius: 10px !important; font-size: 16px !important;
}
.stButton>button {
    background-color: #4e9af1; color: white; border-radius: 8px; height: 3em; width: 100%;
    font-size: 18px; font-weight: 600; transition: 0.3s;
}
.stButton>button:hover {background-color: #1976d2; transform: scale(1.03);}
.result-box {border-radius: 12px; padding: 1.5em; text-align: center; font-size: 20px; font-weight: 600; margin-top: 20px;}
</style>
""", unsafe_allow_html=True)

# --- Sidebar ---
st.sidebar.image("https://cdn-icons-png.flaticon.com/512/4727/4727425.png", width=80)
st.sidebar.title("üéì Nh√≥m 10 - ML Project")
st.sidebar.write("**·ª®ng d·ª•ng Ph√¢n t√≠ch c·∫£m x√∫c t·ª´ Nhi·ªÅu N·ªÅn T·∫£ng** ü§ñ  \nFrontend hi·ªÉn th·ªã k·∫øt qu·∫£ c·∫£m x√∫c v√† th·ªëng k√™ tr·ª±c quan.")
st.sidebar.markdown("---")
st.sidebar.info("üí° Ch·ªâ h·ªó tr·ª£ YouTube / Reddit. M√¥ h√¨nh ch·∫°y local (kh√¥ng c·∫ßn API).")

# --- Ti√™u ƒë·ªÅ ---
st.title("üß† Ph√¢n t√≠ch c·∫£m x√∫c b√¨nh lu·∫≠n ƒëa n·ªÅn t·∫£ng")

# --- Nh·∫≠p link ---
link = st.text_input(f"üîó D√°n link Youtube ho·∫∑c Reddit t·∫°i ƒë√¢y:", placeholder="V√≠ d·ª•: https://www.youtube.com/watch?v=... ho·∫∑c https://www.reddit.com/r/...")

# --- Radio 4 ch·∫ø ƒë·ªô ---
option = st.radio(
    "üìä Ch·ªçn ch·ª©c nƒÉng b·∫°n mu·ªën hi·ªÉn th·ªã:",
    ["T·∫•t c·∫£ th·ªëng k√™", "Th·ªëng k√™ c∆° b·∫£n", "Ph√¢n b·ªë & T·ª∑ l·ªá", "WordCloud theo c·∫£m x√∫c"],
    horizontal=True
)

# --- c√°c h√†m v·∫Ω t·ª´ng c·ª•m ---
def draw_basic_stats(content, authors, labels):
    comments_by = {"T√≠ch c·ª±c": [], "Trung l·∫≠p": [], "Ti√™u c·ª±c": []}
    for c, lab in zip(content, labels):
        comments_by[lab].append(c)

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    # 1) Count
    counts = {k: len(v) for k, v in comments_by.items()}
    axes[0].bar(counts.keys(), counts.values(), color=["#66BB6A", "#BDBDBD", "#EF5350"])
    axes[0].set_title("S·ªë l∆∞·ª£ng comment theo c·∫£m x√∫c"); axes[0].set_ylabel("S·ªë l∆∞·ª£ng")

    # 2) Avg length
    avg_len = {k: (np.mean([len(x) for x in v]) if v else 0) for k, v in comments_by.items()}
    axes[1].bar(avg_len.keys(), avg_len.values(), color=["#66BB6A", "#BDBDBD", "#EF5350"])
    axes[1].set_title("ƒê·ªô d√†i b√¨nh lu·∫≠n trung b√¨nh"); axes[1].set_ylabel("K√Ω t·ª±")

    # 3) Top 5 t√°c gi·∫£
    from collections import Counter
    top_authors = Counter(authors).most_common(5)
    names, values = zip(*top_authors) if top_authors else ([], [])
    axes[2].bar(names, values, color="#42A5F5")
    axes[2].set_title("Top 5 t√°c gi·∫£ b√¨nh lu·∫≠n"); axes[2].set_ylabel("S·ªë comment")
    plt.tight_layout(); st.pyplot(fig); plt.close(fig)

def draw_distribution_and_share(content, authors, labels):
    comments_by = {"T√≠ch c·ª±c": [], "Trung l·∫≠p": [], "Ti√™u c·ª±c": []}
    for c, lab in zip(content, labels):
        comments_by[lab].append(c)

    col1, col2, col3 = st.columns(3)

    with col1:
        fig2, ax2 = plt.subplots(figsize=(5.8, 5))
        sizes = [len(v) for v in comments_by.values()]
        ax2.pie(sizes, labels=list(comments_by.keys()), autopct="%1.1f%%",
                colors=["#66BB6A", "#BDBDBD", "#EF5350"], startangle=90, textprops={"fontsize": 10})
        ax2.set_title("T·ª∑ l·ªá c·∫£m x√∫c"); st.pyplot(fig2); plt.close(fig2)

    with col2:
        fig3, ax3 = plt.subplots(figsize=(5.8, 5.2))
        for name, color in zip(["T√≠ch c·ª±c", "Trung l·∫≠p", "Ti√™u c·ª±c"], ["#66BB6A", "#BDBDBD", "#EF5350"]):
            lengths = [len(c) for c in comments_by[name]]
            if lengths:
                ax3.hist(lengths, bins=10, alpha=0.55, label=name, color=color)
        ax3.set_xlabel("ƒê·ªô d√†i (k√Ω t·ª±)"); ax3.set_ylabel("S·ªë l∆∞·ª£ng"); ax3.legend()
        ax3.set_title("Ph√¢n b·ªë ƒë·ªô d√†i b√¨nh lu·∫≠n"); st.pyplot(fig3); plt.close(fig3)

    with col3:
        from collections import Counter
        top = Counter(authors).most_common(5)
        names = [x[0] for x in top]
        pos_vals, neu_vals, neg_vals = [], [], []
        for name in names:
            pos = sum(1 for a, lab in zip(authors, labels) if a == name and lab == "T√≠ch c·ª±c")
            neu = sum(1 for a, lab in zip(authors, labels) if a == name and lab == "Trung l·∫≠p")
            neg = sum(1 for a, lab in zip(authors, labels) if a == name and lab == "Ti√™u c·ª±c")
            pos_vals.append(pos); neu_vals.append(neu); neg_vals.append(neg)
        fig4, ax4 = plt.subplots(figsize=(5.8, 5))
        ax4.bar(names, pos_vals, label="T√≠ch c·ª±c", color="#66BB6A")
        ax4.bar(names, neu_vals, bottom=pos_vals, label="Trung l·∫≠p", color="#BDBDBD")
        ax4.bar(names, neg_vals, bottom=[i + j for i, j in zip(pos_vals, neu_vals)], label="Ti√™u c·ª±c", color="#EF5350")
        ax4.set_ylabel("S·ªë comment"); ax4.set_title("Top 5 t√°c gi·∫£ theo nh√£n")
        plt.xticks(rotation=30, ha="right"); ax4.legend()
        st.pyplot(fig4); plt.close(fig4)

def draw_wordclouds(content, labels):
    comments_by = {"T√≠ch c·ª±c": [], "Trung l·∫≠p": [], "Ti√™u c·ª±c": []}
    for c, lab in zip(content, labels):
        comments_by[lab].append(c)

    c1, c2, c3 = st.columns(3)
    for (name, cmap, col) in [("T√≠ch c·ª±c", "Greens", c1), ("Trung l·∫≠p", "Greys", c2), ("Ti√™u c·ª±c", "Reds", c3)]:
        with col:
            text = " ".join(comments_by[name])
            if text.strip():
                wc = WordCloud(width=420, height=300, background_color="white", colormap=cmap).generate(text)
                fig, ax = plt.subplots(figsize=(6.5, 4))
                ax.imshow(wc, interpolation="bilinear"); ax.axis("off"); ax.set_title(f"WordCloud: {name}")
                st.pyplot(fig); plt.close(fig)
            else:
                st.info(f"Kh√¥ng c√≥ comment {name}.")
# ---------------------------------------------------------------

# --- N√∫t ch√≠nh ---
if st.button("üöÄ B·∫Øt ƒë·∫ßu ph√¢n t√≠ch"):
    if not link.strip():
        st.warning("‚ö†Ô∏è Vui l√≤ng d√°n link tr∆∞·ªõc khi ph√¢n t√≠ch.")
    else:
        with st.spinner("üîÑ ƒêang t·∫£i v√† ph√¢n t√≠ch b√¨nh lu·∫≠n t·ª´ link..."):
            time.sleep(0.5)

            content, authors = [], []
            link_lc = link.lower()
            is_yt = ("youtube.com" in link_lc) or ("youtu.be" in link_lc)
            is_reddit = "reddit.com" in link_lc

            if not (is_yt or is_reddit):
                st.error("Ch·ªâ h·ªó tr·ª£ YouTube ho·∫∑c Reddit. Vui l√≤ng ki·ªÉm tra link.")
                st.stop()

            if _CRAWLER_OK:
                try:
                    if is_yt:
                        cmts = Crawler(link)
                        cmts.get_youtube_comments()
                        content = [c.get("text", "") for c in cmts.comments if isinstance(c, dict)]
                        authors = [c.get("author", "Unknown") for c in cmts.comments if isinstance(c, dict)]
                    else:
                        cr = CrawlReddit()
                        rows = cr.get_comments(link)  # list (author, text)
                        for a, t in rows:
                            authors.append(a or "Unknown")
                            content.append(t or "")
                except Exception as e:
                    st.error(f"L·ªói crawl: {e}")
                    content, authors = [], []
            else:
                st.warning("Kh√¥ng import ƒë∆∞·ª£c crawler t·ª´ d·ª± √°n. B·∫°n c√≥ th·ªÉ b·∫≠t crawler (c√†i lib & .env) ho·∫∑c d√°n comment th·ªß c√¥ng ·ªü b·∫£n demo kh√°c.")
                # v·∫´n ch·∫∑n n·∫øu r·ªóng
            if not content:
                st.warning("Kh√¥ng c√≥ comment h·ª£p l·ªá ƒë·ªÉ ph√¢n t√≠ch! (Thi·∫øu API key ho·∫∑c link kh√¥ng h·ª£p l·ªá)")
                st.stop()

            # Suy lu·∫≠n local b·∫±ng model .h5
            try:
                labels = predict_local(content)  # ["T√≠ch c·ª±c" / "Trung l·∫≠p" / "Ti√™u c·ª±c"]
            except Exception as e:
                st.error(f"L·ªói suy lu·∫≠n model local: {e}")
                st.stop()

            # Hi·ªÉn th·ªã theo 4 ch·∫ø ƒë·ªô
            if option == "T·∫•t c·∫£ th·ªëng k√™":
                st.success(f"üí¨ ƒê√£ ph√¢n t√≠ch {len(content)} b√¨nh lu·∫≠n t·ª´ link.")
                draw_basic_stats(content, authors, labels)
                st.markdown("---")
                draw_distribution_and_share(content, authors, labels)
                st.markdown("---")
                st.subheader("WordCloud theo c·∫£m x√∫c")
                draw_wordclouds(content, labels)

            elif option == "Th·ªëng k√™ c∆° b·∫£n":
                st.success(f"üí¨ {len(content)} b√¨nh lu·∫≠n ¬∑ Th·ªëng k√™ c∆° b·∫£n")
                draw_basic_stats(content, authors, labels)

            elif option == "Ph√¢n b·ªë & T·ª∑ l·ªá":
                st.success(f"üí¨ {len(content)} b√¨nh lu·∫≠n ¬∑ Ph√¢n b·ªë & T·ª∑ l·ªá")
                draw_distribution_and_share(content, authors, labels)

            elif option == "WordCloud theo c·∫£m x√∫c":
                st.success(f"üí¨ {len(content)} b√¨nh lu·∫≠n ¬∑ WordCloud")
                draw_wordclouds(content, labels)

# --- Footer ---
st.markdown("---")
st.markdown("üß© *D·ª± √°n Machine Learning - Ph√¢n t√≠ch c·∫£m x√∫c b√¨nh lu·∫≠n ƒëa n·ªÅn t·∫£ng Nh√≥m 10 (2025)*")
# ======================
